Training tuneprune_lambda_0.0005_bs_18_acc_6_sparsity_0.63 on Python code
Input model: /n/netscratch/iaifi_lab/Lab/ericjm/narrow/tuneprune_pruned/lambda_0.0005_bs_18_acc_6_sparsity_0.63
Output: /n/netscratch/iaifi_lab/Lab/ericjm/narrow/python_trained/tuneprune_lambda_0.0005_bs_18_acc_6_sparsity_0.63

================================================================================
Training Pruned Model on Python Code
================================================================================
Model: /n/netscratch/iaifi_lab/Lab/ericjm/narrow/tuneprune_pruned/lambda_0.0005_bs_18_acc_6_sparsity_0.63
Output: /n/netscratch/iaifi_lab/Lab/ericjm/narrow/python_trained/tuneprune_lambda_0.0005_bs_18_acc_6_sparsity_0.63
Max steps: 10000
Save steps: 2500
Convert to variable size: True
================================================================================

Loading model...
Converting to VariableSizeLlamaForCausalLM...
  Original parameters: 1,235,814,400
  New parameters: 728,473,600
  Reduction: 41.1%
Loading Python code dataset...
Tokenizing dataset...

Starting training...
